{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this notebook, it is time to play with ML. I'm gonna use some tree based algorithms (xgboost and lgbm), some neural nets, and bayesian optimization to hyperparameter tunning. Hope to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hugo_\\\\OneDrive\\\\Documentos\\\\DataScience\\\\Repos\\\\kaggle_credit_risk\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utils \n",
    "os.chdir('C:\\\\Users\\\\hugo_\\\\OneDrive\\\\Documentos\\\\DataScience\\\\Repos\\\\kaggle_credit_risk\\\\code')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# Data directory\n",
    "os.chdir('C:\\\\Users\\\\hugo_\\\\OneDrive\\\\Documentos\\\\DataScience\\\\Repos\\\\kaggle_credit_risk\\\\data\\\\treated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_eng.csv')\n",
    "test = pd.read_csv('test_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>...</th>\n",
       "      <th>EXT_SOURCE_2^3</th>\n",
       "      <th>EXT_SOURCE_2^2 EXT_SOURCE_3</th>\n",
       "      <th>EXT_SOURCE_2^2 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_2 EXT_SOURCE_3^2</th>\n",
       "      <th>EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_2 DAYS_BIRTH^2</th>\n",
       "      <th>EXT_SOURCE_3^3</th>\n",
       "      <th>EXT_SOURCE_3^2 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_3 DAYS_BIRTH^2</th>\n",
       "      <th>DAYS_BIRTH^3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018181</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>-654.152107</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>-346.733022</td>\n",
       "      <td>2.353667e+07</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>-183.785678</td>\n",
       "      <td>1.247560e+07</td>\n",
       "      <td>-8.468590e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240927</td>\n",
       "      <td>0.197797</td>\n",
       "      <td>-6491.237078</td>\n",
       "      <td>0.162388</td>\n",
       "      <td>-5329.192190</td>\n",
       "      <td>1.748916e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-4375.173647</td>\n",
       "      <td>1.435830e+08</td>\n",
       "      <td>-4.712058e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171798</td>\n",
       "      <td>0.225464</td>\n",
       "      <td>-5885.942404</td>\n",
       "      <td>0.295894</td>\n",
       "      <td>-7724.580288</td>\n",
       "      <td>2.016572e+08</td>\n",
       "      <td>0.388325</td>\n",
       "      <td>-10137.567875</td>\n",
       "      <td>2.646504e+08</td>\n",
       "      <td>-6.908939e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275185</td>\n",
       "      <td>0.216129</td>\n",
       "      <td>-8040.528832</td>\n",
       "      <td>0.169746</td>\n",
       "      <td>-6314.981929</td>\n",
       "      <td>2.349331e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-4959.747997</td>\n",
       "      <td>1.845150e+08</td>\n",
       "      <td>-6.864416e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>0.053210</td>\n",
       "      <td>-2076.117157</td>\n",
       "      <td>0.084225</td>\n",
       "      <td>-3286.224555</td>\n",
       "      <td>1.282190e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-5201.667828</td>\n",
       "      <td>2.029540e+08</td>\n",
       "      <td>-7.918677e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
       "0      100002       1             0          202500.0    406597.5   \n",
       "1      100003       0             0          270000.0   1293502.5   \n",
       "2      100004       0             0           67500.0    135000.0   \n",
       "3      100006       0             0          135000.0    312682.5   \n",
       "4      100007       0             0          121500.0    513000.0   \n",
       "\n",
       "   AMT_ANNUITY  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  \\\n",
       "0      24700.5         351000.0                    0.018801       -9461   \n",
       "1      35698.5        1129500.0                    0.003541      -16765   \n",
       "2       6750.0         135000.0                    0.010032      -19046   \n",
       "3      29686.5         297000.0                    0.008019      -19005   \n",
       "4      21865.5         513000.0                    0.028663      -19932   \n",
       "\n",
       "   DAYS_EMPLOYED  ...  EXT_SOURCE_2^3  EXT_SOURCE_2^2 EXT_SOURCE_3  \\\n",
       "0           -637  ...        0.018181                     0.009637   \n",
       "1          -1188  ...        0.240927                     0.197797   \n",
       "2           -225  ...        0.171798                     0.225464   \n",
       "3          -3039  ...        0.275185                     0.216129   \n",
       "4          -3038  ...        0.033616                     0.053210   \n",
       "\n",
       "   EXT_SOURCE_2^2 DAYS_BIRTH  EXT_SOURCE_2 EXT_SOURCE_3^2  \\\n",
       "0                -654.152107                     0.005108   \n",
       "1               -6491.237078                     0.162388   \n",
       "2               -5885.942404                     0.295894   \n",
       "3               -8040.528832                     0.169746   \n",
       "4               -2076.117157                     0.084225   \n",
       "\n",
       "   EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH  EXT_SOURCE_2 DAYS_BIRTH^2  \\\n",
       "0                           -346.733022               2.353667e+07   \n",
       "1                          -5329.192190               1.748916e+08   \n",
       "2                          -7724.580288               2.016572e+08   \n",
       "3                          -6314.981929               2.349331e+08   \n",
       "4                          -3286.224555               1.282190e+08   \n",
       "\n",
       "   EXT_SOURCE_3^3  EXT_SOURCE_3^2 DAYS_BIRTH  EXT_SOURCE_3 DAYS_BIRTH^2  \\\n",
       "0        0.002707                -183.785678               1.247560e+07   \n",
       "1        0.133318               -4375.173647               1.435830e+08   \n",
       "2        0.388325              -10137.567875               2.646504e+08   \n",
       "3        0.133318               -4959.747997               1.845150e+08   \n",
       "4        0.133318               -5201.667828               2.029540e+08   \n",
       "\n",
       "   DAYS_BIRTH^3  \n",
       "0 -8.468590e+11  \n",
       "1 -4.712058e+12  \n",
       "2 -6.908939e+12  \n",
       "3 -6.864416e+12  \n",
       "4 -7.918677e+12  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was previously treated and engineered (by hand, since automated feature engineering was quite expansive and my computer couldn't handle it). Let's begin the ML process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing validation process\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "# Ensemble of trees classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "# Decision tree just for visualization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Our validation metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Importing lgbm and xgboost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like on Fail Fast, I'll use some algorithms with default parameters. Then, I'll use xgboost, lgbm and neural nets. And for the best model, I'll tune hyperparameters with Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "bc = BaggingClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# classifiers dicts\n",
    "classifiers = {\n",
    "    'Decision Tree': dt,\n",
    "    'Random Forest': rf,\n",
    "    'AdaBoost': ada,\n",
    "    'Bagging Classifier': bc,\n",
    "    'Extra Tree Classifier': etc,\n",
    "    'Gradient Boosting': gbc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a validation method, I'll use Stratified Shuffle Split. It creates n different splits on the data, shuffling the samples. This method is necessary in order to find out if our algorithm is overfitting in the training data. A good algorithm should be able to perform well on data not previously seen in the training process. This is called generalization, and it is the key points of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Let's make a for loop iteration over classifiers and getting metrics. But first, I'll drop the ID column and target column from the training and testing dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_train = train.SK_ID_CURR.values\n",
    "ID_test = test.SK_ID_CURR.values\n",
    "\n",
    "y_train = train.TARGET.values\n",
    "\n",
    "X_train = train.drop(['SK_ID_CURR', 'TARGET'], axis=1).values\n",
    "X_test = test.drop(['SK_ID_CURR'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Decision Tree\n",
      "-------------------\n",
      "[0.539914882234249, 0.539380105635225, 0.5400495700880246, 0.5406251134834923, 0.5475818608028813]\n",
      "AUC: 0.5415103064487745 +/- 0.0030615101994478666\n",
      "Time elapsed: 6 minutes and 55 seconds\n",
      "====================\n",
      "Classifier: Random Forest\n",
      "-------------------\n",
      "[0.6413934611027277, 0.6486663273968813, 0.6504979055497364, 0.649072114803174, 0.6494542883827652]\n",
      "AUC: 0.6478168194470569 +/- 0.003268837402287209\n",
      "Time elapsed: 3 minutes and 9 seconds\n",
      "====================\n",
      "Classifier: AdaBoost\n",
      "-------------------\n",
      "[0.7464020613082756, 0.7574549705694789, 0.7592203730209601, 0.7467749389470386, 0.7537020176123673]\n",
      "AUC: 0.752710872291624 +/- 0.005308461885649792\n",
      "Time elapsed: 16 minutes and 19 seconds\n",
      "====================\n",
      "Classifier: Bagging Classifier\n",
      "-------------------\n",
      "[0.646937282402861, 0.6487526407417195, 0.6475231546214709, 0.6504828468379277, 0.6317010074392173]\n",
      "AUC: 0.6450793864086393 +/- 0.006798455507822953\n",
      "Time elapsed: 44 minutes and 5 seconds\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Extra Tree Classifier\n",
      "-------------------\n",
      "[0.6542854565027115, 0.6393832584194475, 0.6525781305305951, 0.6507107861563624, 0.6511619064112601]\n",
      "AUC: 0.6496239076040753 +/- 0.0052702013675929435\n",
      "Time elapsed: 1 minutes and 46 seconds\n",
      "====================\n",
      "Classifier: Gradient Boosting\n",
      "-------------------\n",
      "[0.7647890191132665, 0.7649323475618499, 0.7483139620885462, 0.7570576171815314, 0.7612006862270643]\n",
      "AUC: 0.7592587264344516 +/- 0.006183495292752015\n",
      "Time elapsed: 50 minutes and 8 seconds\n",
      "====================\n",
      "\n",
      "Total time elapsed: 122 minutes and 24 seconds\n"
     ]
    }
   ],
   "source": [
    "tic_model = datetime.datetime.now()\n",
    "for model, clf in classifiers.items():\n",
    "    \n",
    "    roc_aucs = []\n",
    "    \n",
    "    tic_cv = datetime.datetime.now()\n",
    "    for train_index, val_index in sss.split(X_train, y_train):\n",
    "        X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "        X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "        \n",
    "        clf.fit(X_train_, y_train_)\n",
    "        roc_aucs.append(roc_auc_score(y_val_, clf.predict_proba(X_val_)[:,1]))\n",
    "    toc_cv = datetime.datetime.now()\n",
    "    \n",
    "    print('Classifier: ' + model)\n",
    "    print('-------------------')\n",
    "    print(roc_aucs)\n",
    "    print('AUC: {} +/- {}'.format(np.array(roc_aucs).mean(), np.array(roc_aucs).std()))\n",
    "    print(\"Time elapsed: {} minutes and {} seconds\".format(int((toc_cv - tic_cv).seconds / 60), \n",
    "                                                           int((toc_cv - tic_cv).seconds % 60)))\n",
    "    print('='*20)\n",
    "\n",
    "toc_model = datetime.datetime.now()\n",
    "print()   \n",
    "print(\"Total time elapsed: {} minutes and {} seconds\".format(int((toc_model - tic_model).seconds / 60),\n",
    "                                                             int((toc_model - tic_model).seconds % 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good. GradientBoosting had the best performance. Let's submit a file and see what the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new gradient boosting classifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# training\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# creating submission file\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'SK_ID_CURR': ID_test,\n",
    "        'TARGET': gbc.predict_proba(X_test)[:,1]\n",
    "    }\n",
    ").to_csv('..\\\\..\\\\submissions\\\\submission_gbc.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see if the results are satisfatory, let's calculate the AUC on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data is: 0.7648329039998469\n"
     ]
    }
   ],
   "source": [
    "print('AUC on training data is: {}'.format(roc_auc_score(y_train, gbc.predict_proba(X_train)[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! \n",
    "\n",
    "The submission got __0.74089__ on private leaderboard. It is my best result so far. But can I make it better? The awnser is: off course! Let's use better models, such as LightGBM and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "LightGBM has became very popular because it implements gradient boosting machines in such a light way compared to XGBoost and others implementations. It is perfect for people with low RAM (such as me). Let's see how the algorithm works.\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. But how it differs from other tree based algorithms?\n",
    "\n",
    "LightGBM grows trees __vertically__ while other algorithm grows trees horizontally, meaning that LightGBM grows trees __leaf-wise__ while other algorithms grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, leaf-wise algorithms can reduce more loss than a level-wise one.\n",
    "\n",
    "![Explains how LightGBM works](leaf-wise.png)\n",
    "\n",
    "![Explains how other algorithms works](level-wise.png)\n",
    "\n",
    "This algorithm is called light because it requires low memory to run with large data. Also, it supports GPUs, so that's a bonus.\n",
    "\n",
    "Let's check a few important parameters in order to understand that the algorithm is doing.\n",
    "\n",
    "- __boosting_type__ (string, optional (default='gbdt')) â€“ â€˜gbdtâ€™, traditional Gradient Boosting Decision Tree. â€˜dartâ€™, Dropouts meet Multiple Additive Regression Trees. â€˜gossâ€™, Gradient-based One-Side Sampling. â€˜rfâ€™, Random Forest.\n",
    "\n",
    "- __num_leaves__ (int, optional (default=31)) â€“ Maximum tree leaves for base learners.\n",
    "\n",
    "- __max_depth__ (int, optional (default=-1)) â€“ Maximum tree depth for base learners, <=0 means no limit.\n",
    "\n",
    "- __learning_rate__ (float, optional (default=0.1)) â€“ Boosting learning rate. You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback. Note, that this will ignore the learning_rate argument in training.\n",
    "\n",
    "- __n_estimators__ (int, optional (default=100)) â€“ Number of boosted trees to fit.\n",
    "\n",
    "- __subsample_for_bin__ (int, optional (default=200000)) â€“ Number of samples for constructing bins.\n",
    "\n",
    "- __objective__ (string, callable or None, optional (default=None)) â€“ Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below). Default: â€˜regressionâ€™ for LGBMRegressor, â€˜binaryâ€™ or â€˜multiclassâ€™ for LGBMClassifier, â€˜lambdarankâ€™ for LGBMRanker.\n",
    "\n",
    "- __class_weight__ (dict, 'balanced' or None, optional (default=None)) â€“ Weights associated with classes in the form {class_label: weight}. This parameter must be used only for multi-class classification task; for binary classification task you may use __is_unbalance__ or __scale_pos_weight__ parameters. Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities. You may want to consider performing probability calibration (https://scikit-learn.org/stable/modules/calibration.html) of your model. The â€˜balancedâ€™ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n",
    "\n",
    "- __min_split_gain__ (float, optional (default=0.)) â€“ Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "- __min_child_weight__ (float, optional (default=1e-3)) â€“ Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
    "\n",
    "- __min_child_samples__ (int, optional (default=20)) â€“ Minimum number of data needed in a child (leaf).\n",
    "\n",
    "- __subsample__ (float, optional (default=1.)) â€“ Subsample ratio of the training instance.\n",
    "\n",
    "- __subsample_freq__ (int, optional (default=0)) â€“ Frequence of subsample, <=0 means no enable.\n",
    "\n",
    "- __colsample_bytree__ (float, optional (default=1.)) â€“ Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "- __reg_alpha__ (float, optional (default=0.)) â€“ L1 regularization term on weights.\n",
    "\n",
    "- __reg_lambda__ (float, optional (default=0.)) â€“ L2 regularization term on weights.\n",
    "\n",
    "- __random_state__ (int or None, optional (default=None)) â€“ Random number seed. If None, default seeds in C++ code will be used.\n",
    "\n",
    "- __n_jobs__ (int, optional (default=-1)) â€“ Number of parallel threads.\n",
    "\n",
    "- __silent__ (bool, optional (default=True)) â€“ Whether to print messages while running boosting.\n",
    "\n",
    "- __importance_type__ (string, optional (default='split')) â€“ The type of feature importance to be filled into feature_importances_. If â€˜splitâ€™, result contains numbers of times the feature is used in a model. If â€˜gainâ€™, result contains total gains of splits which use the feature.\n",
    "\n",
    "For more information, [this link](https://lightgbm.readthedocs.io/en/latest/Python-API.html) should be helpfull.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.2 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 100,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'num_rounds': 100,\n",
    "        'metric' : 'auc'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\hugo_\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\hugo_\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\hugo_\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\hugo_\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LGBM\n",
      "-------------------\n",
      "[0.7520520985269431, 0.747631775374966, 0.7390433233155147, 0.754214483953404, 0.748605890247179]\n",
      "AUC: 0.7483095142836013 +/- 0.005201635654617002\n",
      "Time elapsed: 1 minutes and 41 seconds\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "tic_cv = datetime.datetime.now()\n",
    "roc_aucs = []\n",
    "for train_index, val_index in sss.split(X_train, y_train):\n",
    "    X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "    X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "\n",
    "    model.fit(X_train_, y_train_)\n",
    "    roc_aucs.append(roc_auc_score(y_val_, model.predict_proba(X_val_)[:,1]))\n",
    "    \n",
    "toc_cv = datetime.datetime.now()\n",
    "\n",
    "print('Classifier: LGBM')\n",
    "print('-------------------')\n",
    "print(roc_aucs)\n",
    "print('AUC: {} +/- {}'.format(np.array(roc_aucs).mean(), np.array(roc_aucs).std()))\n",
    "print(\"Time elapsed: {} minutes and {} seconds\".format(int((toc_cv - tic_cv).seconds / 60), \n",
    "                                                       int((toc_cv - tic_cv).seconds % 60)))\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see good results in a few minutes. Let's create a submission file and see the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugo_\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "# training on full data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# creating submission file\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'SK_ID_CURR': ID_test,\n",
    "        'TARGET': model.predict_proba(X_test)[:,1]\n",
    "    }\n",
    ").to_csv('..\\\\..\\\\submissions\\\\submission_lgbm.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on training data is: 0.9152767085082014\n"
     ]
    }
   ],
   "source": [
    "print('AUC on training data is: {}'.format(roc_auc_score(y_train, model.predict_proba(X_train)[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission made __0.73124__ on private leaderboard. Let's check XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost is an excellent algorithm, it is also an ensemble of trees, optimized by gradient boosting. Its implementation is heavier than LightGBM, but it still can provide good results (top Kagglers use this algorithm a lot!). I'm not gonna dive into the explanation of XGBoost (you will find [here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) some helpfull information). Just keep in mind that it works like LightGBM, except that it grows its trees level-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(n_estimators = 10) # for memory reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_cv = datetime.datetime.now()\n",
    "roc_aucs = []\n",
    "for train_index, val_index in sss.split(X_train, y_train):\n",
    "    X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "    X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "\n",
    "    model.fit(X_train_, y_train_)\n",
    "    roc_aucs.append(roc_auc_score(y_val_, model.predict_proba(X_val_)[:,1]))\n",
    "    \n",
    "toc_cv = datetime.datetime.now()\n",
    "\n",
    "print('Classifier: LGBM')\n",
    "print('-------------------')\n",
    "print(roc_aucs)\n",
    "print('AUC: {} +/- {}'.format(np.array(roc_aucs).mean(), np.array(roc_aucs).std()))\n",
    "print(\"Time elapsed: {} minutes and {} seconds\".format(int((toc_cv - tic_cv).seconds / 60), \n",
    "                                                       int((toc_cv - tic_cv).seconds % 60)))\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on full data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# creating submission file\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'SK_ID_CURR': ID_test,\n",
    "        'TARGET': model.predict_proba(X_test)[:,1]\n",
    "    }\n",
    ").to_csv('..\\\\..\\\\submissions\\\\submission_xgb.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC on training data is: {}'.format(roc_auc_score(y_train, model.predict_proba(X_train)[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen so far that gradient boosting trees are better models for this particular task. In the next kernel I'll be doing hyperparameter tunning with random search and bayesian optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
