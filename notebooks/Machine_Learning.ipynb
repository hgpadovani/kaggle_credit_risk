{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this notebook, it is time to play with ML. I'm gonna use some tree based algorithms (xgboost and lgbm), some neural nets, and bayesian optimization to hyperparameter tunning. Hope to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utils \n",
    "os.chdir('C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\code')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# Data directory\n",
    "os.chdir('C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\data\\\\treated_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_eng.csv')\n",
    "test = pd.read_csv('test_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>...</th>\n",
       "      <th>EXT_SOURCE_2^3</th>\n",
       "      <th>EXT_SOURCE_2^2 EXT_SOURCE_3</th>\n",
       "      <th>EXT_SOURCE_2^2 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_2 EXT_SOURCE_3^2</th>\n",
       "      <th>EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_2 DAYS_BIRTH^2</th>\n",
       "      <th>EXT_SOURCE_3^3</th>\n",
       "      <th>EXT_SOURCE_3^2 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_3 DAYS_BIRTH^2</th>\n",
       "      <th>DAYS_BIRTH^3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018181</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>-654.152107</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>-346.733022</td>\n",
       "      <td>2.353667e+07</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>-183.785678</td>\n",
       "      <td>1.247560e+07</td>\n",
       "      <td>-8.468590e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240927</td>\n",
       "      <td>0.197797</td>\n",
       "      <td>-6491.237078</td>\n",
       "      <td>0.162388</td>\n",
       "      <td>-5329.192190</td>\n",
       "      <td>1.748916e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-4375.173647</td>\n",
       "      <td>1.435830e+08</td>\n",
       "      <td>-4.712058e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171798</td>\n",
       "      <td>0.225464</td>\n",
       "      <td>-5885.942404</td>\n",
       "      <td>0.295894</td>\n",
       "      <td>-7724.580288</td>\n",
       "      <td>2.016572e+08</td>\n",
       "      <td>0.388325</td>\n",
       "      <td>-10137.567875</td>\n",
       "      <td>2.646504e+08</td>\n",
       "      <td>-6.908939e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275185</td>\n",
       "      <td>0.216129</td>\n",
       "      <td>-8040.528832</td>\n",
       "      <td>0.169746</td>\n",
       "      <td>-6314.981929</td>\n",
       "      <td>2.349331e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-4959.747997</td>\n",
       "      <td>1.845150e+08</td>\n",
       "      <td>-6.864416e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>0.053210</td>\n",
       "      <td>-2076.117157</td>\n",
       "      <td>0.084225</td>\n",
       "      <td>-3286.224555</td>\n",
       "      <td>1.282190e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-5201.667828</td>\n",
       "      <td>2.029540e+08</td>\n",
       "      <td>-7.918677e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
       "0      100002       1             0          202500.0    406597.5   \n",
       "1      100003       0             0          270000.0   1293502.5   \n",
       "2      100004       0             0           67500.0    135000.0   \n",
       "3      100006       0             0          135000.0    312682.5   \n",
       "4      100007       0             0          121500.0    513000.0   \n",
       "\n",
       "   AMT_ANNUITY  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  \\\n",
       "0      24700.5         351000.0                    0.018801       -9461   \n",
       "1      35698.5        1129500.0                    0.003541      -16765   \n",
       "2       6750.0         135000.0                    0.010032      -19046   \n",
       "3      29686.5         297000.0                    0.008019      -19005   \n",
       "4      21865.5         513000.0                    0.028663      -19932   \n",
       "\n",
       "   DAYS_EMPLOYED  ...  EXT_SOURCE_2^3  EXT_SOURCE_2^2 EXT_SOURCE_3  \\\n",
       "0           -637  ...        0.018181                     0.009637   \n",
       "1          -1188  ...        0.240927                     0.197797   \n",
       "2           -225  ...        0.171798                     0.225464   \n",
       "3          -3039  ...        0.275185                     0.216129   \n",
       "4          -3038  ...        0.033616                     0.053210   \n",
       "\n",
       "   EXT_SOURCE_2^2 DAYS_BIRTH  EXT_SOURCE_2 EXT_SOURCE_3^2  \\\n",
       "0                -654.152107                     0.005108   \n",
       "1               -6491.237078                     0.162388   \n",
       "2               -5885.942404                     0.295894   \n",
       "3               -8040.528832                     0.169746   \n",
       "4               -2076.117157                     0.084225   \n",
       "\n",
       "   EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH  EXT_SOURCE_2 DAYS_BIRTH^2  \\\n",
       "0                           -346.733022               2.353667e+07   \n",
       "1                          -5329.192190               1.748916e+08   \n",
       "2                          -7724.580288               2.016572e+08   \n",
       "3                          -6314.981929               2.349331e+08   \n",
       "4                          -3286.224555               1.282190e+08   \n",
       "\n",
       "   EXT_SOURCE_3^3  EXT_SOURCE_3^2 DAYS_BIRTH  EXT_SOURCE_3 DAYS_BIRTH^2  \\\n",
       "0        0.002707                -183.785678               1.247560e+07   \n",
       "1        0.133318               -4375.173647               1.435830e+08   \n",
       "2        0.388325              -10137.567875               2.646504e+08   \n",
       "3        0.133318               -4959.747997               1.845150e+08   \n",
       "4        0.133318               -5201.667828               2.029540e+08   \n",
       "\n",
       "   DAYS_BIRTH^3  \n",
       "0 -8.468590e+11  \n",
       "1 -4.712058e+12  \n",
       "2 -6.908939e+12  \n",
       "3 -6.864416e+12  \n",
       "4 -7.918677e+12  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was previously treated and engineered (by hand, since automated feature engineering was quite expansive and my computer couldn't handle it). Let's begin the ML process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing validation process\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "\n",
    "# Ensemble of trees classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "# Decision tree just for visualization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Our validation metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Importing lgbm and xgboost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like on Fail Fast, I'll use some algorithms with default parameters. Then, I'll use xgboost, lgbm and neural nets. And for the best model, I'll tune hyperparameters with Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "bc = BaggingClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# classifiers dicts\n",
    "classifiers = {\n",
    "    'Decision Tree': dt,\n",
    "    'Random Forest': rf,\n",
    "    'AdaBoost': ada,\n",
    "    'Bagging Classifier': bc,\n",
    "    'Extra Tree Classifier': etc,\n",
    "    'Gradient Boosting': gbc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a validation method, I'll use Stratified Shuffle Split. It creates n different splits on the data, shuffling the samples. This method is necessary in order to find out if our algorithm is overfitting in the training data. A good algorithm should be able to perform well on data not previously seen in the training process. This is called generalization, and it is the key points of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Let's make a for loop iteration over classifiers and getting metrics. But first, I'll drop the ID column and target column from the training and testing dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_train = train.SK_ID_CURR.values\n",
    "ID_test = test.SK_ID_CURR.values\n",
    "\n",
    "y_train = train.TARGET.values\n",
    "\n",
    "X_train = train.drop(['SK_ID_CURR', 'TARGET'], axis=1).values\n",
    "X_test = test.drop(['SK_ID_CURR'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Decision Tree\n",
      "-------------------\n",
      "[0.539914882234249, 0.539380105635225, 0.5400495700880246, 0.5406251134834923, 0.5475818608028813]\n",
      "AUC: 0.5415103064487745 +/- 0.0030615101994478666\n",
      "Time elapsed: 6 minutes and 55 seconds\n",
      "====================\n",
      "Classifier: Random Forest\n",
      "-------------------\n",
      "[0.6413934611027277, 0.6486663273968813, 0.6504979055497364, 0.649072114803174, 0.6494542883827652]\n",
      "AUC: 0.6478168194470569 +/- 0.003268837402287209\n",
      "Time elapsed: 3 minutes and 9 seconds\n",
      "====================\n",
      "Classifier: AdaBoost\n",
      "-------------------\n",
      "[0.7464020613082756, 0.7574549705694789, 0.7592203730209601, 0.7467749389470386, 0.7537020176123673]\n",
      "AUC: 0.752710872291624 +/- 0.005308461885649792\n",
      "Time elapsed: 16 minutes and 19 seconds\n",
      "====================\n",
      "Classifier: Bagging Classifier\n",
      "-------------------\n",
      "[0.646937282402861, 0.6487526407417195, 0.6475231546214709, 0.6504828468379277, 0.6317010074392173]\n",
      "AUC: 0.6450793864086393 +/- 0.006798455507822953\n",
      "Time elapsed: 44 minutes and 5 seconds\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Extra Tree Classifier\n",
      "-------------------\n",
      "[0.6542854565027115, 0.6393832584194475, 0.6525781305305951, 0.6507107861563624, 0.6511619064112601]\n",
      "AUC: 0.6496239076040753 +/- 0.0052702013675929435\n",
      "Time elapsed: 1 minutes and 46 seconds\n",
      "====================\n",
      "Classifier: Gradient Boosting\n",
      "-------------------\n",
      "[0.7647890191132665, 0.7649323475618499, 0.7483139620885462, 0.7570576171815314, 0.7612006862270643]\n",
      "AUC: 0.7592587264344516 +/- 0.006183495292752015\n",
      "Time elapsed: 50 minutes and 8 seconds\n",
      "====================\n",
      "\n",
      "Total time elapsed: 122 minutes and 24 seconds\n"
     ]
    }
   ],
   "source": [
    "tic_model = datetime.datetime.now()\n",
    "for model, clf in classifiers.items():\n",
    "    \n",
    "    roc_aucs = []\n",
    "    \n",
    "    tic_cv = datetime.datetime.now()\n",
    "    for train_index, val_index in sss.split(X_train, y_train):\n",
    "        X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "        X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "        \n",
    "        clf.fit(X_train_, y_train_)\n",
    "        roc_aucs.append(roc_auc_score(y_val_, clf.predict_proba(X_val_)[:,1]))\n",
    "    toc_cv = datetime.datetime.now()\n",
    "    \n",
    "    print('Classifier: ' + model)\n",
    "    print('-------------------')\n",
    "    print(roc_aucs)\n",
    "    print('AUC: {} +/- {}'.format(np.array(roc_aucs).mean(), np.array(roc_aucs).std()))\n",
    "    print(\"Time elapsed: {} minutes and {} seconds\".format(int((toc_cv - tic_cv).seconds / 60), \n",
    "                                                           int((toc_cv - tic_cv).seconds % 60)))\n",
    "    print('='*20)\n",
    "\n",
    "toc_model = datetime.datetime.now()\n",
    "print()   \n",
    "print(\"Total time elapsed: {} minutes and {} seconds\".format(int((toc_model - tic_model).seconds / 60),\n",
    "                                                             int((toc_model - tic_model).seconds % 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good. GradientBoosting had the best performance. I believe it could be improved by using better algorithms, such as XGBoost and LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "LightGBM has became very popular because it implements gradient boosting machines in such a light way compared to XGBoost and others implementations. It is perfect for people with low RAM (such as me). Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.2 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 100,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'num_rounds': 100,\n",
    "        'metric' : 'auc'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LGBM\n",
      "-------------------\n",
      "[0.7542896207992694, 0.7570891165304523, 0.7509195466310534, 0.75516050442667, 0.7462740123946162]\n",
      "AUC: 0.7527465601564123 +/- 0.0038025731261801137\n",
      "Time elapsed: 2 minutes and 11 seconds\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "tic_cv = datetime.datetime.now()\n",
    "roc_aucs = []\n",
    "for train_index, val_index in sss.split(X_train, y_train):\n",
    "    X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "    X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "\n",
    "    model.fit(X_train_, y_train_)\n",
    "    roc_aucs.append(roc_auc_score(y_val_, model.predict_proba(X_val_)[:,1]))\n",
    "    \n",
    "toc_cv = datetime.datetime.now()\n",
    "\n",
    "print('Classifier: LGBM')\n",
    "print('-------------------')\n",
    "print(roc_aucs)\n",
    "print('AUC: {} +/- {}'.format(np.array(roc_aucs).mean(), np.array(roc_aucs).std()))\n",
    "print(\"Time elapsed: {} minutes and {} seconds\".format(int((toc_cv - tic_cv).seconds / 60), \n",
    "                                                       int((toc_cv - tic_cv).seconds % 60)))\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see good results in a few minutes. I think this is the best algorithm for us to work with. Let's try to optimize its parameters with Bayesian Optimization using the hyperopt library and the Tree Parzen Estimators (TPE) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization for LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define our search space for LGBM. The hyperparameters must be defined as some sort of propabilistic distribution. They are the priors that serves as input to our bayesian optimization algorithm. The hyperopt library offers some default distributions, such as uniform, log uniform and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "from hyperopt import hp\n",
    "\n",
    "space_lgbm = {\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our search space, in which the Tree Parzen Estimators algorithm is going to make some assumptions about the probabilistic surface it will optimize, let's create a file to follow up the results of the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, Trials, STATUS_OK, fmin, space_eval\n",
    "import csv\n",
    "\n",
    "# optimization algorithm\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Keep track of results\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# create the lgbm Dataset\n",
    "train_set = lgb.Dataset(X_train, label = y_train)\n",
    "\n",
    "# Output file\n",
    "out_file = 'gbm_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Than, in order to our algorithm to work, it is necessary to create a function it will optimize. I'll be writing specifically for LGBM, but later in this notebook I'll write a more generic function that supports other algorithms. \n",
    "\n",
    "This function must return at least 2 parameters: the loss, which is the objective to optimize, and a flag called STATUS_OK, which serves as a go on flag. The other parameters returned are optional, but is interesting to keep track of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "def objective_lgbm(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Retrieve the subsample if present otherwise set to 1.0\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "     # Extract the best score\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'estimators': n_estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all set up, let's call the fmin function, which will call the objective_lgbm and the search space it will covers to optimize.\n",
    "\n",
    "This algorithms works as follows:\n",
    "- Build a surrogate probability model of the objective function\n",
    "- Find the hyperparameters that perform best on the surrogate\n",
    "- Apply these hyperparameters to the true objective function\n",
    "- Update the surrogate model incorporating the new results\n",
    "- Repeat steps 2–4 until max iterations or time is reached\n",
    "\n",
    "If you want to dig deeper into this process, read the [excelent article](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f) on the subject and the [original paper](https://app.sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [19:38<6:45:28, 256.09s/it, best loss: 0.2330311008445396]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global variable\n",
    "global  ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "MAX_EVALS = 100\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective_lgbm, space = space_lgbm, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': {'boosting_type': 'goss', 'subsample': 1.0},\n",
       " 'class_weight': 'balanced',\n",
       " 'colsample_bytree': 0.7847366868237033,\n",
       " 'learning_rate': 0.022060926302396058,\n",
       " 'min_child_samples': 190.0,\n",
       " 'num_leaves': 105.0,\n",
       " 'reg_alpha': 0.830079188734248,\n",
       " 'reg_lambda': 0.8610765653882214,\n",
       " 'subsample_for_bin': 280000.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_eval(space_lgbm, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.22288786482334866,\n",
       "  'params': {'boosting_type': 'goss',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.7847366868237033,\n",
       "   'learning_rate': 0.022060926302396058,\n",
       "   'min_child_samples': 190,\n",
       "   'num_leaves': 105,\n",
       "   'reg_alpha': 0.830079188734248,\n",
       "   'reg_lambda': 0.8610765653882214,\n",
       "   'subsample_for_bin': 280000,\n",
       "   'subsample': 1.0},\n",
       "  'iteration': 41,\n",
       "  'estimators': 32,\n",
       "  'train_time': 0.3195206219997999,\n",
       "  'status': 'ok'},\n",
       " {'loss': 0.22427035330261136,\n",
       "  'params': {'boosting_type': 'dart',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.8567164455106517,\n",
       "   'learning_rate': 0.011726613176605479,\n",
       "   'min_child_samples': 185,\n",
       "   'num_leaves': 116,\n",
       "   'reg_alpha': 0.9133969680917509,\n",
       "   'reg_lambda': 0.12344041899551565,\n",
       "   'subsample_for_bin': 280000,\n",
       "   'subsample': 0.787226858089079},\n",
       "  'iteration': 68,\n",
       "  'estimators': 37,\n",
       "  'train_time': 103.57025506900027,\n",
       "  'status': 'ok'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>iteration</th>\n",
       "      <th>estimators</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.222888</td>\n",
       "      <td>{'boosting_type': 'goss', 'class_weight': 'bal...</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>0.319521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.224270</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "      <td>68</td>\n",
       "      <td>37</td>\n",
       "      <td>103.570255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.224654</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "      <td>66</td>\n",
       "      <td>33</td>\n",
       "      <td>98.669069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.225384</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>113.597497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.226190</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "      <td>28</td>\n",
       "      <td>196</td>\n",
       "      <td>95.640259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params  iteration  \\\n",
       "0  0.222888  {'boosting_type': 'goss', 'class_weight': 'bal...         41   \n",
       "1  0.224270  {'boosting_type': 'dart', 'class_weight': 'bal...         68   \n",
       "2  0.224654  {'boosting_type': 'dart', 'class_weight': 'bal...         66   \n",
       "3  0.225384  {'boosting_type': 'dart', 'class_weight': 'bal...         71   \n",
       "4  0.226190  {'boosting_type': 'dart', 'class_weight': 'bal...         28   \n",
       "\n",
       "   estimators  train_time  \n",
       "0          32    0.319521  \n",
       "1          37  103.570255  \n",
       "2          33   98.669069  \n",
       "3          37  113.597497  \n",
       "4         196   95.640259  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('gbm_trials.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the results are saved into a csv file, it gets converted into a string. Let's use the ast function to convert it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'goss',\n",
       " 'class_weight': 'balanced',\n",
       " 'colsample_bytree': 0.7847366868237033,\n",
       " 'learning_rate': 0.022060926302396058,\n",
       " 'min_child_samples': 190,\n",
       " 'num_leaves': 105,\n",
       " 'reg_alpha': 0.830079188734248,\n",
       " 'reg_lambda': 0.8610765653882214,\n",
       " 'subsample_for_bin': 280000,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to evaluate out results. Hope to get good AUC with this tunned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='goss', class_weight='balanced',\n",
       "        colsample_bytree=0.7847366868237033, importance_type='split',\n",
       "        learning_rate=0.022060926302396058, max_depth=-1,\n",
       "        min_child_samples=190, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=32, n_jobs=-1, num_leaves=105, objective='binary',\n",
       "        random_state=50, reg_alpha=0.830079188734248,\n",
       "        reg_lambda=0.8610765653882214, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=280000, subsample_freq=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators = best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating submission file\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'SK_ID_CURR': ID_test,\n",
    "        'TARGET': best_bayes_model.predict_proba(X_test)[:,1]\n",
    "    }\n",
    ").to_csv('submission_lgbm_tunned.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Bayesian Optimization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be writing a function that accepts scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    \"\"\"Objective function for Generic Model Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "        \n",
    "    # KFold cross-validation\n",
    "    kf = StratifiedKFold(n_splits = 5)\n",
    "    \n",
    "    # Getting the model\n",
    "    model = space['model'](**space['params'])\n",
    "\n",
    "    # Starting\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    roc_aucs = []\n",
    "    for train_index, val_index in kf.split(X_train[:1000,:], y_train[:1000]):\n",
    "        X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "        X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "\n",
    "        # Change the model here\n",
    "        model.fit(X_train_, y_train_)\n",
    "        roc_aucs.append(roc_auc_score(y_val_, model.predict_proba(X_val_)[:,1]))\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "     # Extract the best score\n",
    "    best_score = np.mean(roc_aucs)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, space, ITERATION, run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': space, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'status': STATUS_OK}\n",
    "\n",
    "def optimize(objective, space, trials, MAX_EVALS = 120, output_file = 'BayesOpt.csv', random_state = 42):\n",
    "    \n",
    "    # Output file\n",
    "    out_file = output_file\n",
    "    of_connection = open(out_file, 'w')\n",
    "    writer = csv.writer(of_connection)\n",
    "\n",
    "    # Write the headers to the file\n",
    "    writer.writerow(['loss', 'params', 'iteration', 'train_time'])\n",
    "    of_connection.close()\n",
    "    \n",
    "    # Global variable\n",
    "    global  ITERATION\n",
    "\n",
    "    ITERATION = 0\n",
    "\n",
    "    # Run optimization\n",
    "    best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "                max_evals = MAX_EVALS, trials = trials, rstate = np.random.RandomState(42))\n",
    "    \n",
    "    return space_eval(space, best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utils \n",
    "os.chdir('C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\spaces')\n",
    "\n",
    "from search_spaces import *\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': sklearn.ensemble.gradient_boosting.GradientBoostingClassifier,\n",
       " 'params': {'learning_rate': <hyperopt.pyll.base.Apply at 0x1c609c04d68>,\n",
       "  'subsample': <hyperopt.pyll.base.Apply at 0x1c609c04eb8>,\n",
       "  'max_depth': <hyperopt.pyll.base.Apply at 0x1c609c0a080>,\n",
       "  'max_features': <hyperopt.pyll.base.Apply at 0x1c609c0a198>,\n",
       "  'min_samples_leaf': <hyperopt.pyll.base.Apply at 0x1c609c0a400>,\n",
       "  'min_samples_split': <hyperopt.pyll.base.Apply at 0x1c609c0a588>}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unknown type of parameter:boosting_type, got:dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6aca44232121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrf_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbest_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgm_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-a3b4b1cb0a84>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(objective, space, trials, MAX_EVALS, output_file, random_state)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m# Run optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     best = fmin(fn = objective, space = space, algo = tpe.suggest, \n\u001b[1;32m---> 63\u001b[1;33m                 max_evals = MAX_EVALS, trials = trials, rstate = np.random.RandomState(42))\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mspace_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         )\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[0;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    225\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                         \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 844\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-a3b4b1cb0a84>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(space)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Change the model here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mroc_aucs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    745\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[0;32m   1530\u001b[0m                 raise TypeError('Training data should be Dataset instance, met {}'\n\u001b[0;32m   1531\u001b[0m                                 .format(type(train_set).__name__))\n\u001b[1;32m-> 1532\u001b[1;33m             \u001b[0mparams_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_dict_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1533\u001b[0m             \u001b[1;31m# set network if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0malias\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"machines\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"workers\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"nodes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mparam_dict_to_str\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise TypeError('Unknown type of parameter:%s, got:%s'\n\u001b[1;32m--> 132\u001b[1;33m                             % (key, type(val).__name__))\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unknown type of parameter:boosting_type, got:dict"
     ]
    }
   ],
   "source": [
    "rf_trials = Trials()\n",
    "\n",
    "best_rf = optimize(objective, lgm_space, rf_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
