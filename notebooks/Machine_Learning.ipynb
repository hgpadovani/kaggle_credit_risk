{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "In this notebook, it is time to play with ML. I'm gonna use some tree based algorithms (xgboost and lgbm), some neural nets, and bayesian optimization to hyperparameter tunning. Hope to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utils \n",
    "os.chdir('C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\code')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# Data directory\n",
    "os.chdir('C:\\\\Users\\\\Hugo\\\\Desktop\\\\hugo\\\\DataScience\\\\Kaggle\\\\kaggle_credit_risk\\\\data\\\\treated_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_eng.csv')\n",
    "test = pd.read_csv('test_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>...</th>\n",
       "      <th>EXT_SOURCE_2^3</th>\n",
       "      <th>EXT_SOURCE_2^2 EXT_SOURCE_3</th>\n",
       "      <th>EXT_SOURCE_2^2 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_2 EXT_SOURCE_3^2</th>\n",
       "      <th>EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_2 DAYS_BIRTH^2</th>\n",
       "      <th>EXT_SOURCE_3^3</th>\n",
       "      <th>EXT_SOURCE_3^2 DAYS_BIRTH</th>\n",
       "      <th>EXT_SOURCE_3 DAYS_BIRTH^2</th>\n",
       "      <th>DAYS_BIRTH^3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018181</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>-654.152107</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>-346.733022</td>\n",
       "      <td>2.353667e+07</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>-183.785678</td>\n",
       "      <td>1.247560e+07</td>\n",
       "      <td>-8.468590e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240927</td>\n",
       "      <td>0.197797</td>\n",
       "      <td>-6491.237078</td>\n",
       "      <td>0.162388</td>\n",
       "      <td>-5329.192190</td>\n",
       "      <td>1.748916e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-4375.173647</td>\n",
       "      <td>1.435830e+08</td>\n",
       "      <td>-4.712058e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171798</td>\n",
       "      <td>0.225464</td>\n",
       "      <td>-5885.942404</td>\n",
       "      <td>0.295894</td>\n",
       "      <td>-7724.580288</td>\n",
       "      <td>2.016572e+08</td>\n",
       "      <td>0.388325</td>\n",
       "      <td>-10137.567875</td>\n",
       "      <td>2.646504e+08</td>\n",
       "      <td>-6.908939e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275185</td>\n",
       "      <td>0.216129</td>\n",
       "      <td>-8040.528832</td>\n",
       "      <td>0.169746</td>\n",
       "      <td>-6314.981929</td>\n",
       "      <td>2.349331e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-4959.747997</td>\n",
       "      <td>1.845150e+08</td>\n",
       "      <td>-6.864416e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>0.053210</td>\n",
       "      <td>-2076.117157</td>\n",
       "      <td>0.084225</td>\n",
       "      <td>-3286.224555</td>\n",
       "      <td>1.282190e+08</td>\n",
       "      <td>0.133318</td>\n",
       "      <td>-5201.667828</td>\n",
       "      <td>2.029540e+08</td>\n",
       "      <td>-7.918677e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
       "0      100002       1             0          202500.0    406597.5   \n",
       "1      100003       0             0          270000.0   1293502.5   \n",
       "2      100004       0             0           67500.0    135000.0   \n",
       "3      100006       0             0          135000.0    312682.5   \n",
       "4      100007       0             0          121500.0    513000.0   \n",
       "\n",
       "   AMT_ANNUITY  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  \\\n",
       "0      24700.5         351000.0                    0.018801       -9461   \n",
       "1      35698.5        1129500.0                    0.003541      -16765   \n",
       "2       6750.0         135000.0                    0.010032      -19046   \n",
       "3      29686.5         297000.0                    0.008019      -19005   \n",
       "4      21865.5         513000.0                    0.028663      -19932   \n",
       "\n",
       "   DAYS_EMPLOYED  ...  EXT_SOURCE_2^3  EXT_SOURCE_2^2 EXT_SOURCE_3  \\\n",
       "0           -637  ...        0.018181                     0.009637   \n",
       "1          -1188  ...        0.240927                     0.197797   \n",
       "2           -225  ...        0.171798                     0.225464   \n",
       "3          -3039  ...        0.275185                     0.216129   \n",
       "4          -3038  ...        0.033616                     0.053210   \n",
       "\n",
       "   EXT_SOURCE_2^2 DAYS_BIRTH  EXT_SOURCE_2 EXT_SOURCE_3^2  \\\n",
       "0                -654.152107                     0.005108   \n",
       "1               -6491.237078                     0.162388   \n",
       "2               -5885.942404                     0.295894   \n",
       "3               -8040.528832                     0.169746   \n",
       "4               -2076.117157                     0.084225   \n",
       "\n",
       "   EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH  EXT_SOURCE_2 DAYS_BIRTH^2  \\\n",
       "0                           -346.733022               2.353667e+07   \n",
       "1                          -5329.192190               1.748916e+08   \n",
       "2                          -7724.580288               2.016572e+08   \n",
       "3                          -6314.981929               2.349331e+08   \n",
       "4                          -3286.224555               1.282190e+08   \n",
       "\n",
       "   EXT_SOURCE_3^3  EXT_SOURCE_3^2 DAYS_BIRTH  EXT_SOURCE_3 DAYS_BIRTH^2  \\\n",
       "0        0.002707                -183.785678               1.247560e+07   \n",
       "1        0.133318               -4375.173647               1.435830e+08   \n",
       "2        0.388325              -10137.567875               2.646504e+08   \n",
       "3        0.133318               -4959.747997               1.845150e+08   \n",
       "4        0.133318               -5201.667828               2.029540e+08   \n",
       "\n",
       "   DAYS_BIRTH^3  \n",
       "0 -8.468590e+11  \n",
       "1 -4.712058e+12  \n",
       "2 -6.908939e+12  \n",
       "3 -6.864416e+12  \n",
       "4 -7.918677e+12  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was previously treated and engineered (by hand, since automated feature engineering was quite expansive and my computer couldn't handle it). Let's begin the ML process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing validation process\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Ensemble of trees classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "# Decision tree just for visualization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Our validation metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Importing lgbm and xgboost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like on Fail Fast, I'll use some algorithms with default parameters. Then, I'll use xgboost, lgbm and neural nets. And for the best model, I'll tune hyperparameters with Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "bc = BaggingClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# classifiers dicts\n",
    "classifiers = {\n",
    "    'Decision Tree': dt,\n",
    "    'Random Forest': rf,\n",
    "    'AdaBoost': ada,\n",
    "    'Bagging Classifier': bc,\n",
    "    'Extra Tree Classifier': etc,\n",
    "    'Gradient Boosting': gbc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a validation method, I'll use Stratified Shuffle Split. It creates n different splits on the data, shuffling the samples. This method is necessary in order to find out if our algorithm is overfitting in the training data. A good algorithm should be able to perform well on data not previously seen in the training process. This is called generalization, and it is the key points of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Let's make a for loop iteration over classifiers and getting metrics. But first, I'll drop the ID column and target column from the training and testing dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_train = train.SK_ID_CURR.values\n",
    "ID_test = test.SK_ID_CURR.values\n",
    "\n",
    "y_train = train.TARGET.values\n",
    "\n",
    "X_train = train.drop(['SK_ID_CURR', 'TARGET'], axis=1).values\n",
    "X_test = test.drop(['SK_ID_CURR'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Decision Tree\n",
      "-------------------\n",
      "[0.539914882234249, 0.539380105635225, 0.5400495700880246, 0.5406251134834923, 0.5475818608028813]\n",
      "AUC: 0.5415103064487745 +/- 0.0030615101994478666\n",
      "Time elapsed: 6 minutes and 55 seconds\n",
      "====================\n",
      "Classifier: Random Forest\n",
      "-------------------\n",
      "[0.6413934611027277, 0.6486663273968813, 0.6504979055497364, 0.649072114803174, 0.6494542883827652]\n",
      "AUC: 0.6478168194470569 +/- 0.003268837402287209\n",
      "Time elapsed: 3 minutes and 9 seconds\n",
      "====================\n",
      "Classifier: AdaBoost\n",
      "-------------------\n",
      "[0.7464020613082756, 0.7574549705694789, 0.7592203730209601, 0.7467749389470386, 0.7537020176123673]\n",
      "AUC: 0.752710872291624 +/- 0.005308461885649792\n",
      "Time elapsed: 16 minutes and 19 seconds\n",
      "====================\n",
      "Classifier: Bagging Classifier\n",
      "-------------------\n",
      "[0.646937282402861, 0.6487526407417195, 0.6475231546214709, 0.6504828468379277, 0.6317010074392173]\n",
      "AUC: 0.6450793864086393 +/- 0.006798455507822953\n",
      "Time elapsed: 44 minutes and 5 seconds\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Extra Tree Classifier\n",
      "-------------------\n",
      "[0.6542854565027115, 0.6393832584194475, 0.6525781305305951, 0.6507107861563624, 0.6511619064112601]\n",
      "AUC: 0.6496239076040753 +/- 0.0052702013675929435\n",
      "Time elapsed: 1 minutes and 46 seconds\n",
      "====================\n",
      "Classifier: Gradient Boosting\n",
      "-------------------\n",
      "[0.7647890191132665, 0.7649323475618499, 0.7483139620885462, 0.7570576171815314, 0.7612006862270643]\n",
      "AUC: 0.7592587264344516 +/- 0.006183495292752015\n",
      "Time elapsed: 50 minutes and 8 seconds\n",
      "====================\n",
      "\n",
      "Total time elapsed: 122 minutes and 24 seconds\n"
     ]
    }
   ],
   "source": [
    "tic_model = datetime.datetime.now()\n",
    "for model, clf in classifiers.items():\n",
    "    \n",
    "    roc_aucs = []\n",
    "    \n",
    "    tic_cv = datetime.datetime.now()\n",
    "    for train_index, val_index in sss.split(X_train, y_train):\n",
    "        X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "        X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "        \n",
    "        clf.fit(X_train_, y_train_)\n",
    "        roc_aucs.append(roc_auc_score(y_val_, clf.predict_proba(X_val_)[:,1]))\n",
    "    toc_cv = datetime.datetime.now()\n",
    "    \n",
    "    print('Classifier: ' + model)\n",
    "    print('-------------------')\n",
    "    print(roc_aucs)\n",
    "    print('AUC: {} +/- {}'.format(np.array(roc_aucs).mean(), np.array(roc_aucs).std()))\n",
    "    print(\"Time elapsed: {} minutes and {} seconds\".format(int((toc_cv - tic_cv).seconds / 60), \n",
    "                                                           int((toc_cv - tic_cv).seconds % 60)))\n",
    "    print('='*20)\n",
    "\n",
    "toc_model = datetime.datetime.now()\n",
    "print()   \n",
    "print(\"Total time elapsed: {} minutes and {} seconds\".format(int((toc_model - tic_model).seconds / 60),\n",
    "                                                             int((toc_model - tic_model).seconds % 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's good. GradientBoosting had the best performance. I believe it could be improved by using better algorithms, such as XGBoost and LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "LightGBM has became very popular because it implements gradient boosting machines in such a light way compared to XGBoost and others implementations. It is perfect for people with low RAM (such as me). Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.2 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 100,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 256,\n",
    "        'num_rounds': 100,\n",
    "        'metric' : 'auc'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LGBM\n",
      "-------------------\n",
      "[0.7542896207992694, 0.7570891165304523, 0.7509195466310534, 0.75516050442667, 0.7462740123946162]\n",
      "AUC: 0.7527465601564123 +/- 0.0038025731261801137\n",
      "Time elapsed: 2 minutes and 11 seconds\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "tic_cv = datetime.datetime.now()\n",
    "roc_aucs = []\n",
    "for train_index, val_index in sss.split(X_train, y_train):\n",
    "    X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "    X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "\n",
    "    model.fit(X_train_, y_train_)\n",
    "    roc_aucs.append(roc_auc_score(y_val_, model.predict_proba(X_val_)[:,1]))\n",
    "    \n",
    "toc_cv = datetime.datetime.now()\n",
    "\n",
    "print('Classifier: LGBM')\n",
    "print('-------------------')\n",
    "print(roc_aucs)\n",
    "print('AUC: {} +/- {}'.format(np.array(roc_aucs).mean(), np.array(roc_aucs).std()))\n",
    "print(\"Time elapsed: {} minutes and {} seconds\".format(int((toc_cv - tic_cv).seconds / 60), \n",
    "                                                       int((toc_cv - tic_cv).seconds % 60)))\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see good results in a few minutes. I think this is the best algorithm for us to work with. Let's try to optimize its parameters with Bayesian Optimization using the hyperopt library and the Tree Parzen Estimators (TPE) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization for LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define our search space for LGBM. The hyperparameters must be defined as some sort of propabilistic distribution. They are the priors that serves as input to our bayesian optimization algorithm. The hyperopt library offers some default distributions, such as uniform, log uniform and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "from hyperopt import hp\n",
    "\n",
    "space_lgbm = {\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our search space, in which the Tree Parzen Estimators algorithm is going to make some assumptions about the probabilistic surface it will optimize, let's create a file to follow up the results of the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, Trials, STATUS_OK, fmin, space_eval\n",
    "import csv\n",
    "\n",
    "# optimization algorithm\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Keep track of results\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# create the lgbm Dataset\n",
    "train_set = lgb.Dataset(X_train, label = y_train)\n",
    "\n",
    "# Output file\n",
    "out_file = 'gbm_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Than, in order to our algorithm to work, it is necessary to create a function it will optimize. I'll be writing specifically for LGBM, but later in this notebook I'll write a more generic function that supports other algorithms. \n",
    "\n",
    "This function must return at least 2 parameters: the loss, which is the objective to optimize, and a flag called STATUS_OK, which serves as a go on flag. The other parameters returned are optional, but is interesting to keep track of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "def objective_lgbm(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Retrieve the subsample if present otherwise set to 1.0\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "     # Extract the best score\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'estimators': n_estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all set up, let's call the fmin function, which will call the objective_lgbm and the search space it will covers to optimize.\n",
    "\n",
    "This algorithms works as follows:\n",
    "- Build a surrogate probability model of the objective function\n",
    "- Find the hyperparameters that perform best on the surrogate\n",
    "- Apply these hyperparameters to the true objective function\n",
    "- Update the surrogate model incorporating the new results\n",
    "- Repeat steps 2â€“4 until max iterations or time is reached\n",
    "\n",
    "If you want to dig deeper into this process, read the [excelent article](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f) on the subject and the [original paper](https://app.sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                    \n",
      "353.33398968300025                                   \n",
      "2                                                                                \n",
      "729.0066054769995                                                                \n",
      "3                                                                                 \n",
      "168.95101494099072                                                                \n",
      "4                                                                                 \n",
      "185.21730174600089                                                                \n",
      "5                                                                                 \n",
      "1196.1440229180007                                                               \n",
      "6                                                                                 \n",
      "  5%|â–Œ         | 5/100 [43:52<15:22:13, 582.46s/it, best loss: 0.2330311008445396]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\Anaconda3\\lib\\site-packages\\lightgbm\\callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
      "  warnings.warn('Early stopping is not available in dart mode')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global variable\n",
    "global  ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "MAX_EVALS = 100\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective_lgbm, space = space_lgbm, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a97749e4fc71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Sort the trials with lowest loss (highest AUC) first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbayes_trials_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbayes_trials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbayes_trials_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-a97749e4fc71>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Sort the trials with lowest loss (highest AUC) first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbayes_trials_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbayes_trials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbayes_trials_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('gbm_trials.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the results are saved into a csv file, it gets converted into a string. Let's use the ast function to convert it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to evaluate out results. Hope to get good AUC with this tunned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators = best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating submission file\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'SK_ID_CURR': ID_test,\n",
    "        'TARGET': best_bayes_model.predict_proba(X_test)[:,1]\n",
    "    }\n",
    ").to_csv('../submissions/Submission_lgbm_tunned.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Bayesian Optimization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be writing a function that accepts scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    \"\"\"Objective function for Generic Model Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "        \n",
    "    # KFold cross-validation\n",
    "    kf = StratifiedKFold(n_splits = 5)\n",
    "    \n",
    "    # Getting the model\n",
    "    model = space['model'](**space['params'])\n",
    "\n",
    "    # Starting\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    roc_aucs = []\n",
    "    for train_index, val_index in kf.split(X_train, y_train):\n",
    "        X_train_, y_train_ = X_train[train_index, :], y_train[train_index]\n",
    "        X_val_, y_val_ = X_train[val_index, :], y_train[val_index]\n",
    "\n",
    "        # Change the model here\n",
    "        model.fit(X_train_, y_train_)\n",
    "        roc_aucs.append(roc_auc_score(y_val_, model.predict_proba(X_val_)[:,1]))\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "     # Extract the best score\n",
    "    best_score = np.mean(roc_aucs)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'status': STATUS_OK}\n",
    "\n",
    "def optimize(objective, space, MAX_EVALS = 120, trials, output_file = 'BayesOpt.csv', random_state = 42):\n",
    "    \n",
    "    # Output file\n",
    "    out_file = output_file\n",
    "    of_connection = open(out_file, 'w')\n",
    "    writer = csv.writer(of_connection)\n",
    "\n",
    "    # Write the headers to the file\n",
    "    writer.writerow(['loss', 'params', 'iteration', 'train_time'])\n",
    "    of_connection.close()\n",
    "    \n",
    "    # Global variable\n",
    "    global  ITERATION\n",
    "\n",
    "    ITERATION = 0\n",
    "\n",
    "    # Run optimization\n",
    "    best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "                max_evals = MAX_EVALS, trials = trials, rstate = random_state)\n",
    "    \n",
    "    return space_eval(space, best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
